{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d43e5ee2",
   "metadata": {},
   "source": [
    "# Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9c8e7a",
   "metadata": {},
   "source": [
    "This notebook show how to work with the platform. The possibles action to perform are:\n",
    "- Deploy the Infrastructure locally with docker-compose.\n",
    "- Start the main DAG on Airflow to scrap the site and populate the PostgreSQL Database.\n",
    "- Consume the data for analytics purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b0b0b2",
   "metadata": {},
   "source": [
    "## Before Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c8597f",
   "metadata": {},
   "source": [
    "Is recomended to create a virtual environment to execute this workshop.\n",
    "- Install Python on your machine\n",
    "- Instal virtual env lib with **pip install virtualenv**\n",
    "- Create an env with **python3 -m venv .venv**\n",
    "- Active the environment depending on your OS\n",
    "- Run **pip install -r requirements.txt** insite the new environment\n",
    "- Run **jupyter notebook** in a terminal activated by the new environment\n",
    "- Open **Workshop.ipynb** file to leanr about the platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b1128f",
   "metadata": {},
   "source": [
    "## Deploy Infra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14704e73",
   "metadata": {},
   "source": [
    "To deploy the infra locally, is necessary to have **docker** and **docker-compose** correctly configured locally.\n",
    "After, is possible to start the deploy running the cell bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1622b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec200729",
   "metadata": {},
   "source": [
    "Or is also possible to follow the example and use the deploy_infra_locally method to star."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b22e67c",
   "metadata": {},
   "source": [
    "## Cayena Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d373348",
   "metadata": {},
   "source": [
    "A simple Class was developed to help the Data Scientis to work with the plataform, providing a simple way to interact with the principal services. All the necessaries variables are harde coded at the __init__ method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a3cdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime as dt\n",
    "import requests\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "class Cayena():\n",
    "    def __init__(self, user='airflow', password='airflow'):\n",
    "        self.AUTH        = (user, password)\n",
    "        self.DB_USER     = \"cayena\"\n",
    "        self.DB_DATABASE = \"cayena\"\n",
    "        self.DB_PASS     = \"cayena\"\n",
    "        self.DB_HOST     = \"localhost\"\n",
    "        self.DB_PORT     = 5432\n",
    "        self.BASE_URL    = \"http://localhost:8080/api/v1\"\n",
    "        self.DAG_ID      = \"web_scraping_pipeline\"\n",
    "        \n",
    "    def _connect(self):\n",
    "        self.connection = psycopg2.connect(\n",
    "            database=config.DB_DATABASE,\n",
    "            user=config.DB_USER,\n",
    "            password=config.DB_PASS,\n",
    "            host=config.DB_HOST,\n",
    "            port=config.DB_PORT)\n",
    "        self.cursor = self.connection.cursor()\n",
    "\n",
    "        \n",
    "    def start_web_scraping_dag(self):\n",
    "        url = \"http://localhost:8080/api/v1/dags/web_scraping_pipeline/dagRuns\"\n",
    "\n",
    "        payload = \"{}\"\n",
    "        headers = {\n",
    "            'content-type': \"application/json\"\n",
    "        }\n",
    "\n",
    "        response = requests.request(\n",
    "            \"POST\",\n",
    "            self.BASE_URL + f'/dags/{self.DAG_ID}/dagRuns',\n",
    "            data=payload, headers=headers, auth=self.AUTH)\n",
    "        return response\n",
    "    \n",
    "    def check_dag_status(self):\n",
    "        return json.loads(\n",
    "            requests.request(\n",
    "                \"GET\",\n",
    "                self.BASE_URL + f'/dags/{self.DAG_ID}/dagRuns',\n",
    "                auth=self.AUTH).text\n",
    "        )\n",
    "    \n",
    "    def deploy_infra_locally(self):\n",
    "        os.system('docker-compose up -d')\n",
    "        \n",
    "    def get_books_table_as_df(self):\n",
    "        sql = \"SELECT * FROM cayena.analytics.books\"\n",
    "        return self.get_query_result_as_df(sql)\n",
    "    \n",
    "    def get_query_result_as_df(self, sql):\n",
    "        return pd.read_sql_query(sql, con=self.connection)\n",
    "    \n",
    "    def stop_workshop(self):\n",
    "        os.system('docker-compose down --remove-orphans')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b36184",
   "metadata": {},
   "source": [
    "### Instantiate Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faad81d1",
   "metadata": {},
   "source": [
    "When creating the object from our Class, we can interpretate as the User requesting access to the platform to manage it, so an Auth method can be requested at this point to provid access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8705537",
   "metadata": {},
   "outputs": [],
   "source": [
    "cayena = Cayena()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffad7b7",
   "metadata": {},
   "source": [
    "### Deploy Infra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90be7f7c",
   "metadata": {},
   "source": [
    "As said before, is necessary to deploy the infrastructure locally in order to execute de current workshop. If you didn't set up you environment before with the **docker-compose** command, you can request the **deploy_infra_locally** method to start all the services locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54d3fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cayena.deploy_infra_locally()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c77ad41",
   "metadata": {},
   "source": [
    "### Execute DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca787c5",
   "metadata": {},
   "source": [
    "In order to populate the table **books** with the informations provided by the fake Web Site, we can use the method **start_web_scraping_dag** to trigger our main DAG using the Airflow API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424bfba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cayena.start_web_scraping_dag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5952de01",
   "metadata": {},
   "source": [
    "### Check DAG execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1f61dd",
   "metadata": {},
   "source": [
    "Using the method **check_dag_status** is possible to retrive all the tries to execute the DAG and check whats is the current state of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703932b9",
   "metadata": {},
   "source": [
    "### Query to DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea908d",
   "metadata": {},
   "source": [
    "If you desire to fetch all the data from the **books** table, is possible to use the **get_books_table_as_df** method. Or if the whish is to run a personalized query, the method **get_query_result_as_df** is what you are lokking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9a2b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cayena.get_books_table_as_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9ac5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cayena.get_query_result_as_df(\"SELECT * FROM cayena.analytics.books LIMIT 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2517e5",
   "metadata": {},
   "source": [
    "### Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df72ea66",
   "metadata": {},
   "source": [
    "After finish all the work, you can shut down the infrastructure runnig the cell bellow or executing the method **stop_workshop**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1a5d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker-compose down --remove-orphans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17d8424",
   "metadata": {},
   "outputs": [],
   "source": [
    "cayena.stop_workshop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
